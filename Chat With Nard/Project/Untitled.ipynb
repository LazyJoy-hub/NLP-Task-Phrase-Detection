{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus=list()\n",
    "df=pd.read_csv('../data/training_data.tsv',delimiter='\\t')\n",
    "df=df.dropna()\n",
    "y=df['label']\n",
    "X=df['sent']\n",
    "X_list=list()\n",
    "for _ in X:\n",
    "    t=re.findall('[a-z]+',_.lower())\n",
    "    t=[word for word in t]\n",
    "    X_list+=t\n",
    "X_list=set(X_list)\n",
    "y_list=list()\n",
    "for _ in y:\n",
    "    t=re.findall('[a-z]+',_.lower())\n",
    "    t=[word for word in t]\n",
    "    y_list+=t\n",
    "y_list=set(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus+=list(X_list-y_list)\n",
    "cfile=open(\"mycorpus.pickle\",'wb')\n",
    "pickle.dump(mycorpus,cfile)\n",
    "cfile.close()\n",
    "\n",
    "#Saving mycorpus in file\n",
    "file = open('mycorpus.txt','w')\n",
    "for _ in mycorpus:\n",
    "    file.write(_+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data\n",
    "df=pd.read_csv(\"../data/training_data.tsv\",delimiter='\\t')\n",
    "df=df.dropna()\n",
    "y=df['label']\n",
    "X=df['sent']\n",
    "\n",
    "#Processing Data of X\n",
    "x=list()\n",
    "for _ in X:\n",
    "    t=re.findall('[a-z]+',_.lower())\n",
    "    t=[word for word in t if not word in set(stopwords.words('english'))]\n",
    "    x.append(\" \".join(t))\n",
    "x=pd.Series(x)\n",
    "\n",
    "#processing data of y\n",
    "Y=list()\n",
    "for _ in y:\n",
    "    if _ ==\"Not Found\":\n",
    "        Y.append(\"Not Found\")\n",
    "    else:\n",
    "        Y.append(\"Found\")\n",
    "y=pd.Series(Y)  \n",
    "\n",
    "#splitting data for train and test\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "\n",
    "#Making list of x\n",
    "x_train_list=list()\n",
    "for i in X_train:\n",
    "    x_train_list.append(i)\n",
    "x_test_list=list()\n",
    "for j in X_test:\n",
    "    x_test_list.append(j)\n",
    "\n",
    "#Vectorizing x\n",
    "vectorizer = CountVectorizer(max_features=1000) # n-grams Bag of word\n",
    "train_data = vectorizer.fit_transform(x_train_list) # expects a list of strings\n",
    "np.asarray(train_data)\n",
    "test_data = vectorizer.fit_transform(x_test_list) # expects a list of strings\n",
    "np.asarray(test_data)\n",
    "\n",
    "#Modelling\n",
    "#split=train_data,test_data,y_train,y_test\n",
    "clf=svm.LinearSVC()\n",
    "clf=clf.fit(train_data,y_train)\n",
    "\n",
    "#Saving Model in pickle (\"model.pickle\")\n",
    "f=open(\"model.pickle\",'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "#Checking Accuracy\n",
    "accuracy=clf.score(test_data,y_test)\n",
    "\n",
    "#Predicting result\n",
    "result=clf.predict(test_data)\n",
    "result_list=list(result)\n",
    "\n",
    "#creating confusion matrix\n",
    "cm=confusion_matrix(y_test,result)\n",
    "\n",
    "# saving file of x_test_list,y_test_list,result_list\n",
    "y_test_list=list(y_test)\n",
    "file=open(\"y_test_result.tsv\",'w')\n",
    "file.write(\"sent\\tlabel\\ttest\\n\")\n",
    "for i in range(len(y_test)):\n",
    "    file.write(x_test_list[i]+'\\t'+y_test_list[i]+'\\t'+result_list[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Loading trained model\n",
    "f=open(\"model.pickle\",'rb')\n",
    "clf=pickle.load(f)\n",
    "\n",
    "#Loading eval file\n",
    "eval_data=open(\"../data/eval_data.txt\",'r')\n",
    "X=list()\n",
    "all_words=list()\n",
    "for lines in eval_data:\n",
    "    all_words+=(lines.strip('\\n').split(' '))\n",
    "    X.append(lines.strip('\\n'))\n",
    "eval_data.close()\n",
    "all_words=list(set(all_words))\n",
    "\n",
    "#Vectorizing and converting to array: X_test\n",
    "vectorizer=CountVectorizer(max_features=1000)\n",
    "X_test=vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "#Predicting Result (found/not found)\n",
    "result=list(clf.predict(X_test))\n",
    "\n",
    "#Setting mycopus.txt\n",
    "mycorpus=['i','me','at','to','date','time','for','tommorow','tonight','today',\\\n",
    "           'sunday','monday','tuesday','wednesday','thursday','friday','saturday',\\\n",
    "           'morning','evening']\n",
    "#For found, extract result text\n",
    "def extract(index):\n",
    "    #Create the extractor function\n",
    "    temp=X[index].lower()\n",
    "    t=re.findall('remi[a-z]+ me? to? (.+?) at',temp)\n",
    "    if len(t)==0:\n",
    "        t=re.findall('rem[a-z]+ me? to? (.+?) on',temp)\n",
    "    if len(t)==0:\n",
    "        t=temp\n",
    "        t+=' .y'\n",
    "        t=re.findall('remi[\\w.]+ (.+?) {}'.format(t.split(\" \")[len(t.split(\" \"))-1]),t)\n",
    "    if len(t)>0:\n",
    "        t=str(t[0]).split()    \n",
    "        t=[word for word in t if not word in mycorpus]\n",
    "    if len(t)>0:\n",
    "        result[index]=\" \".join(t)\n",
    "    else:\n",
    "        result[index]=\"Not Found\"\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if(result[i]==\"Found\"):\n",
    "        extract(i)\n",
    "        \n",
    "#----Saving result------------\n",
    "file=open(\"Result.csv\",'w')\n",
    "file.write(\"sent\\tlabel\\n\")\n",
    "for i in range(len(result)):\n",
    "    file.write(str(X[i])+'\\t'+str(result[i])+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
